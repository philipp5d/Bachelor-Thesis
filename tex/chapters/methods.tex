% !TEX root = ../Thesis.tex
\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Methods}
\label{ch:methods}

%\section{Notes on Notation and Naming Conventions}
%
%In this thesis vectors are represented by lowercase boldface characters
%such as $\mathbf{v}$ and matrices by uppercase boldface letters such
%as $\mathbf{M}$. 
%
%In the context of Natural Language Processing in this thesis the words ``document''
%and ``text'' are used interchangeably and refer to texts of variable
%length such as sentences, paragraphs or whole documents. ``term''
%and ``word'' are used interchangeably as well. Although ``term'' in general
%often refers to an entity within the text, that might be
%composed of several ``words'', for example ``New York'', here both will be used to refer to single words.


%\section{Preprocessing}
%
%One very common way of representing text in a computer is to view
%the text as a sequence of individual characters, a string. However,
%for many methods of information retrieval and machine learning, including
%the ones described subsequently, the units of text we would like to
%deal with are words or terms not sequences of characters. The process
%of extracting words or terms from a sequence of characters is known
%as tokenization \citep{Manning2008prepr}. A first naive approach to tokenization might be to
%split the sequence of characters on every whitespace and regard everything
%in between as a word. This approach, however, can easily lead to unexpected
%results. Consider the example string ``john loves mary.''. The naive
%approach yields the words ``john'', ``loves'' and ``mary.''.
%Note how the last word contains the punctuation character ``.''.
%So even for very easy examples the naive approach does not produce
%expected results. Luckily many more sophisticated algorithms are implemented
%in ready-to-use open-source software packages.
%
%A second common step in preprocessing is the removal of so called
%stop words. Stop words are usually the most commonly used words of
%a language, that do not contain much meaning, e.g. words like ``the'',
%``a'', ``just'' or ``some''. While they are necessary for the
%grammatical structure of a language, they do not convey much information
%about the similarity of documents and can be disregarded in further
%analysis steps \citep{Manning2008prepr}.
%
%Another often used preprocessing method is stemming. Stemming refers
%to the process of mapping (inflected) words to their stems, e.g. mapping
%words such as ``am'', ``is'' and ``are'' to the word ``be''.
%The rationale behind using stemming is that for finding e.g. the topic
%of a text it should not matter which form of the word is present in
%a text, but only that a word is present in a text \citep{Manning2008prepr}.
%
%All three preprocessing steps described above are used in this thesis
%except where noted otherwise. The python package NLTK (natural language
%toolkit) provides many specific implementations of these algorithms
%and was used throughout this thesis. For tokenization the ``TreebankWordTokenizer''
%and for stemming the ``SnowballStemmer'' are used. A list of German stopwords is also taken from the package.
%
%
%\section{Bag of Words}
%
%After preprocessing of a text one of the standard approaches for representing
%documents for information retrieval and machine learning algorithms
%is in the bag of words model. In the bag of words model a text is
%represented by the multiset (bag) of its words. That means all word
%order information is disregarded and only the information whether
%(or how often) a word is present in a text is encoded \citep{Manning2008vect}. (Mit der Art wie das Buch hier zitiert wird bin ich noch nicht gl√ºcklich. Es sollte eigentlich chapter 2 und 6 sein und nicht 2008 a und b.)
%
%In this model documents are represented as fixed length
%feature vectors, where every feature is a kind of word occurrence
%count. In the simplest approach every feature is the term frequency
%$tf(t,d)$ of term $t$ in a document $d$. Where $tf(t,d)$ is
%the occurrence count of word $t$ in document $d$. To compute feature
%vectors for documents in a corpus the vocabulary $V$ of the corpus
%needs to be established first. Documents are represented by a $1\times|V|$
%vector of the values $tf(t,d)$ for all $t\in V$. Assume the corpus
%consists of two documents $d_{1}=$``john loves mary'' and $d_{2}=$``mary
%loves pizza''. The vectors representing the two texts are:
%
%\[
%\textbf{v}_{d_1}=\begin{array}{c}
%1\\
%1\\
%1\\
%0
%\end{array}\ \textbf{v}_{d_2}=\begin{array}{c}
%0\\
%1\\
%1\\
%1
%\end{array}\ \begin{array}{c}
%john\\
%loves\\
%mary\\
%pizza
%\end{array}
%\]
%
%
%This representation obviously has severe limitation, consider for example
%the two sentences $d_{3}=$``john loves mary'' and $d_{4}$=``mary
%loves jon''. Their representation in the bag of words model is equivalent,
%although they express quite different meanings. Still the bag of words
%model is a standard approach for information retrieval problems, as
%it has many desirable properties. For example documents can be expressed
%as a fixed length vector, necessary for many machine learning tasks,
%and distances in the vector space can straight forwardly be interpreted
%as a measure for document dissimilarities. In information retrieval often
%the focus lies on the topic of a text rather than it's specific content.
%Some of the weaknesses of the bag of words approach are then neglectable.
%Consider again the sentences $d_{3}$ and $d_{4}$ and additionally
%the sentence $d_{5}=$''computers automate tasks''. If we are interested
%in the topic of the sentences or in their relevance to each other,
%then the bag of words model will give the reasonable result that $d_{3}$
%and $d_{4}$ are more relevant to each other than to $d_{5}$.


\section{N-Grams}

In order to extend the bag of words representation to capture word
order to some extent n-grams have been proposed. The idea of n-grams
is to treat a sequence of n words as a distinct token or term. For
the sentence $d_{3}$ the list of 2-grams or bigrams is {[}(john,
loves), (loves mary){]} and for $d_{4}$it is {[}(mary, loves), (loves,
john){]}. So the problem of the equivalent representation of $d_{3}$
and $d_{4}$ has been solved by capturing word order information in
small context windows. However, this comes at the cost of a higher
dimensionality of the resulting feature vector as there are more distinct
n-grams in a corpus than single words. In principle the cardinality
of the n-gram vocabulary could be as large as $|V_{n}|=|V_{1}|^{n}$,
where $V_{n}$ is the n-gram vocabulary. However, in natural texts
not all possible n-grams occur and the feature vector dimensionality
grows more moderately than an exponential function with increasing
$n$.   (Vielleicht kommt der Absatz doch wieder raus. Habe n-grams jetzt doch noch gar nicht verwendet.)


\section{Term Frequency - Inverse Document Frequency}

A common problem with the bag of words model is that some terms (even
after filtering stop words) appear often across texts in a corpus,
i.e. have a high term frequency yet do not constitute a good feature
for discrimination between texts. Therefore a scaling factor for the
term frequencies is desired which captures the intuition that words
appearing often in a few texts but rarely in others are good discriminative
features for those texts. Term Frequency - Inverse Document Frequency
(tf-idf) refers to a specific scaling scheme, that downscales the
importance of frequent words, while upscaling the importance of rare
words. 

Term frequency $tf(t,d)$ usually refers to the standard word count.
Inverse document frequency $idf(t,C)$ of term $t$ and corpus $C$
can be computed as 
\[
idf(t,C)=log_{2}(\frac{|C|}{|\{d\in C:t\in d\}|})
\]
where $|C|$ is the total number of documents in the corpus and $|\{d\in C:t\in d\}|$
is the number of documents in which term $t$ appears at least once.

Term frequency - Inverse document frequency $tfidf(t,d,C)$ is then
calculated as 
\[
tfidf(t,d,C)=tf(t,d)\cdot idf(t,C)
\]


tf-idf can be used as a feature for the representation of the document
that is more robust to uninformative changes in the distribution of
common words and more expressive for rare words \citep{Manning2008vect}.


\section{Latent Semantic Analysis}

An often occurring machine learning problem is that very high dimensional
feature vectors, as occurring in bag of words models described above,
tend to generalize poorly in subsequent tasks like classification.
Therefore it is desirable to have a more condensed lower dimensional
feature representation of documents that still captures most of the
variance in the bag of words representation. Latent semantic analysis
(LSA) of \citet{Deerwester1990} in its simplest form takes the plain bag of words
vectors of all documents in the corpus and constructs a term-document
matrix $\textbf{M}$, where $\textbf{M}[i,j]=tf(t_{i},d_{j})$, i.e. row $i$ represents
the relation of term $i$ to all documents, while column $j$ represents
one document and the relation to all it's terms. So column j represents document $d_j$'s vector representation $\textbf{v}_{d_j}$.
\[ 
\begin{matrix} & \textbf{v}_{d_j}\\
& \downarrow\\
\textbf{v}_{t_i}^{T}\rightarrow & \begin{bmatrix}tf(1,1) & \dots & tf(1,|C|)\\
\vdots & \ddots & \vdots\\
tf(|V|,1) & \dots & tf(|V|,|C|)
\end{bmatrix}
\end{matrix}
\]
Singular value decomposition is then used on the term document matrix
$\textbf{M}$, allowing to find a $k$-dimensional ($k<dim(\textbf{M})$) linear subspace
of $\textbf{M}$, $\hat{\textbf{M}}$, that still captures as much of the original information
as possible, i.e. that captures as much of the variance in the original
space as possible. Column $j$ of $\hat{\textbf{M}}$ contains a $k$-dimensional,
approximate representation of the document j's feature vector $\textbf{v}_{d_{j}}$, called $\hat{\textbf{v}}_{d_{j}}$.
The document representations $\hat{\textbf{v}}_{d_j}$ in $\textbf{M}$'s linear subspace
spanned by $\hat{\textbf{M}}$ do no longer have an intuitive interpretation
as word counts, but can still be used as feature vectors for subsequent
ML tasks or can be analyzed for similarity. Deerwester et al. argue
that the resulting features have several appealing properties. The
LSA features can simply be viewed as a noise-reduced version of the
original features, but according to the original paper they can also
better deal with linguistic issues such as synonymy and polysemy.
This works because every original observation (i.e. document) is represented
as a linear combination of $k$ hidden (or latent) semantic concepts.
Terms that often appear together (i.e. are semantically related) will
then be mapped onto similar LSA representations and can thereby for
example capture some aspects of synonymy.

The value of $k$ is a parameter of the model and has to be specified
by the researcher. LSA can also be used with tf-idf features.


\section{Latent Dirichlet Allocation}

A popular probabilistic method for finding latent semantic features
of documents in a corpus is latent dirichlet allocation (LDA). As
with LSA the rationale is that it would be useful to find a shorter
or lower dimensional description for documents in the bag of words
vector space format for subsequent tasks. In the LDA context the latent
features are called topics and texts are represented as probabilistic
mixtures of these topics. A topic is represented by a probability
distribution over words and so a document is represented as a probabilistic
sample from several topic distributions over words. As in the case of LSA the
number of topics $k$ is a parameter and needs to be specified by
the researcher \citep{Blei2003}. 


\section{Word Vectors}

In the standard bag of words model no relationship between words is
encoded, i.e. every word is dissimilar from every other word. Thereby
a lot of information is lost, as e.g. the words ``car'' and ``automobile''
are considered by the system to be as similar to each other as to
the word ``blue''. One way to encode relationships between words
is to embed them in a vector space. Just like the way document similarities
can be computed from their distances in the vector space model, similarities
of words can be computed, if the words are represented as vectors.
A first successful approach to this problem used the LSA model. Here
a single word can be represented as a pseudo document and thereby
be mapped into the LSA space. The resulting vector in the LSA feature
space can be considered a vector representation for the word and be
used for comparisons with other words \citep{Deerwester1990}.

\citet{Bengio2003} introduced a neural language model that uses
word vectors to predict subsequent words in a text and updates the
model as well as the vectors with gradient descent. It seems that
by being useful for prediction of subsequent words, the vectors represent
statistical information about word contexts and capture meaning of
the words to some extent.

Recently \citet{Mikolov2013a} proposed their influential Word2Vec model.
This model utilizes a neural network for the prediction of word vectors,
given other nearby word vectors. However, the authors focused on the
refinement of the word vectors only and were able to simplify the
net substantially, while gaining word vector ``performance'' (i.e.
vectors that perform better on a task designed to measure how well
the vectors capture human intuitions about the words). Their model
works simply with scalar products of input and output word vectors
(it has two vectors per word) and a softmax output layer for normalization.
This model comes in two architectural types: ``Continuous Bag of Words''
(CBOW) and ``Continuous Skip-gram'' (skip-gram). The former predicts
the output vector of a center word, given the $n$ previous and
$n$ subsequent input word vectors. The input vectors of the $2n$
surrounding words are averaged, the scalar product of this average
with every output word vector is computed and a softmax normalization
produces final output probabilities. The skip-gram model utilizes
the input vector of the center word to predict the output vectors
of surrounding words in a left and right context window of maximal
size $w$. The current window size $c$ is sampled uniformly from
$range(1,w)$ at every iteration. This results in $2\cdot c$ predictions
for every center word considered. The model parameters are updated
with gradient descent. After training either only the input word vectors
are used or the input and output word vectors are either averaged
or concatenated to produce the final word vectors used for subsequent
tasks. 

A note on computational cost: The softmax output layer is very inefficient
as it requires the computation of $|V|$ scalar products (every word
in the vocabulary). As a first speed up a hierarchical softmax \citep{Morin2005} is
used as an approximation to the real softmax, that reduces the number
of scalar product computations to $log_{2}(|V|)$ \citep{Mikolov2013a}.
In a subsequent publication the authors introduced a
simplified variant of Noise Contrastive Estimation \citep{Gutmann2012}, called
Negative Sampling, that further reduces the computational complexity,
while still giving useful word vectors \citep{Mikolov2013b}. 

With these architecture and approximation simplifications the Word2Vec
model is able to train on a vast amount of text data (billions of
words) in a matter of hours and thereby produce better word vectors,
than previously possible.


\section{Paragraph Vector - An extension of Word2Vec}

A simple extension of the word2vec model allows to obtain vectors
for sentences or longer passages of text. An additional vector is
introduced for every piece of text, that we regard as a separate entity
(a sentence, a paragraph or a document). This so called paragraph
vector is then used in two different ways in the extensions of CBOW
and skip-gram. In the Distributed Memory (DM) model, an extension
of CBOW, the paragraph vector is used in combination (average or concatenation)
with the vectors of surrounding words to predict a center word. In
the Distributed bag of words model, an extension of skip-gram, the
paragraph embedding is used to predict words randomly sampled from
the paragraph.

After the initial training phase a second step, called inference phase, is necessary to gain paragraph embeddings. A paragraph vector is initialized randomly, the rest of the model is kept fixed and the normal training procedure of word prediction and gradient descent on the paragraph vector is done. Thereby the final document embeddings are produced, that can be used like the embeddings of the methods described above. Note, however, that the time until the model reaches convergence both in the training and in the inference phase is unknown and a number of running epochs for both phases must be specified beforehand \citep{Le2014}.
\end{document}