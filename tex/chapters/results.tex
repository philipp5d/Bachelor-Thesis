% !TEX root = ../Thesis.tex
\begin{document}
\documentclass[Thesis.tex]{subfiles}
\chapter{Results}
\label{ch:results}

Vielleicht sollte das Kapitel eher challenges and solutions heißen. Es sind ja nicht nur results, sondern auch immer die Problemstellungen beschrieben.

\section{Duplicate Detection}

%Exploring our dataset we realized that we had to deal with two duplication
%problems. The first being that several letters were contained more
%than once in our dataset. The second, more challenging problem was
%that for several patients letters of different time points with somewhat
%different content were present. To ensure we would not distort
%our results, because of duplicates present in our test set, we tried
%to identify all duplicates of the two kinds. On first glance at least
%the first problem seems easy. Finding exact duplicates is not a challenging
%task. However, as the letters were made anonymous separately, they
%are not exact copies of each other. Names and other personal information
%were eradicated from the word documents by hand, so extra whitespaces
%and similar subtle differences were introduced. For a subset of 150
%of the letters we manually searched for the duplicates. However, this
%work is tedious, error-prone and does not scale to bigger datasets.
%To semi-automatically find the true duplicates and the follow-up letters
%we used two approaches.
%
%The first method searches two letters for their longest common subsequence
%of characters. If this longest common subsequence exceeds a threshold
%relative to the longer document (e.g. 3 \% of the length of the longer
%document), then the two letters are marked as possibly related. In
%the second approach we use the bag of words representations of the
%documents and their cosine distance in the vector space as an indicator.
%If the distance between two vectors is lower than a threshold (e.g.
%0.2), they are also marked as possibly related.
%
%We start in both methods with high thresholds, manually check the results
%and successively lower them until the false positive rate becomes
%too high. With this procedure we were able to quickly find all the
%duplicate pairs we already found manually before. Additionally we
%found two follow-up pairs among the 150 that we did not identify with
%the manual procedure. The bag of words approach is somewhat more reliable
%and identified almost all the duplicates, before including false positives.
%However, we did not find two other duplicates pairs, that
%the string matching procedure quickly identified. So it seems worth
%the effort to use both methods.
%
%Later we found one more follow-up pair by chance. However, these two
%letters are radically different in content and so this event does
%not undermine our confidence, that we were able to find a sufficient
%portion of the duplicate pairs to not distort our test results. (Will ich diesen letzten Absatz überhaupt drin haben?)


\section{Paragraph Extraction}



\section{Paragraph Classification}



\section{Disease Classification}
We tried to automatically assign to each letter which disease the patient had. This is useful for two reasons. First, people might want to search for all patients with a particular disease (more useful than full text search as diseases can be written many ways.) Second, many interesting pieces of information are not coded into a structured database, but are "hidden" in the free text. With these methods we can find this information.

\section{Letter Similarity}
With the vector embedding methods we are able to get an estimate of the dissimilarity of texts simply through a vector distance. This way we are able to suggest letters that are similar to a reference letter. As it worked better than expected we conducted the experiment detailed below


\end{document}