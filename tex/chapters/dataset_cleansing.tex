% !TEX root = ../Thesis.tex
\begin{document}
\documentclass[Thesis.tex]{subfiles}
\chapter{Dataset Cleansing}
\label{ch:dataset cleansing}


The dataset we acquired from the university hospital contains several duplicate letters. To ensure undistorted test results, we have to identify as many of them as possible. It is clear that finding all of these duplicates manually is not only error-prone, but also very time consuming as in principle $\frac{(n-1)^2 + (n-1)}{2}$ letter comparison have to be made. With $n=307$ this amounts to almost 50,000 comparisons. Personal information, like names and dates of birth, would be helpful for this task, but has been removed during anonymization. Therefore we make use of two semi-automatic procedures for duplicate identification. First we use a longest common subsequence matching method. This method would be sufficient for our problems, if we only had to deal with exact duplicates. However, we face the issue of follow-up letters with modified information in our dataset. To overcome this problem we use a procedure well known in the information retrieval community: The bag of words model for representing texts as vectors, with which it is possible to compute distances between documents. Duplicates are then found by their vector proximity. 
%We realized that several duplicate letters are present in this dataset, however. So to identify all of them we make use of two semi-automatic procedures for identifying duplicates. First we use a longest common subsequence string matching method. Second we represent the documents in the bag of words model, where distances between documents can be computed, and find duplicates by their proximity.


\section{The Bag of Words Model}
The bag of words model represents a text as the multiset (bag) of its words. That means all word
order information is disregarded and only the information how often a word is present in a text is encoded \citep{Manning2008vect}. (!correct! Mit der Art wie das Buch hier zitiert wird bin ich noch nicht glÃ¼cklich. Es sollte eigentlich chapter 2 und 6 sein und nicht 2008 a und b. oder wenigstens a, b, c in derselben Reihenfolge wie die chpaters.)
More concretely in the bag of words model documents are represented as fixed length
feature vectors, where every feature is a word occurrence
count. In the simplest approach every feature is the word or term frequency
tf$(t,d)$ of term $t$ in document $d$. Where tf$(t,d)$ is
the occurrence count of word or term $t$ in document $d$. To compute feature
vectors for documents in a corpus the vocabulary $V$ of the corpus
needs to be established first. Documents are represented by a $1\times|V|$
vector of the values tf$(t,d)$ for all $t\in V$.
To represent text in the bag of words model, however, the text needs to be preprocessed first.

\subsection*{Preprocessing}
In computers, text is most often represented as sequences of characters. The process of extracting words from such a sequence is known as tokenization \citep{Manning2008prepr}. A first naive approach to tokenization might be to
split the sequence of characters on every whitespace and regard everything
in between as a word. This approach, however, can easily lead to unexpected
results. Consider the example string ``The doctor treats the patient.''. The naive
approach yields the words ``The'', ``doctor'', ``treats'', ``the'' and ``patient.''.
Note how the last word contains the punctuation character ``.''.
So even for very easy examples the naive approach does not produce
expected results. Luckily many more sophisticated algorithms are implemented
in ready-to-use open-source software packages.

Some words are more important or representative of a text than others and so it is a useful preprocessing step for the bag of words model to remove some frequently appearing but uninformative words, so called stop words. A list of English stop words comprises words like ``the'', ``a'', ``just'' or ``some''.  While they are necessary for the
grammatical structure of a language, they do not convey much information
about the similarity of documents \citep{Manning2008prepr}.

Additionally for the bag of words model we are not interested in the exact grammatical form in which a word is present in a text. Removing this unnecessary information is achieved with a procedure called stemming, that maps word appearances to their stem \citep{Manning2008prepr}. It maps words such as ``doctors'' and ``doctor'' both to their common stem ``doctor''. Both removal of stop words and stemming seem at first glance to disregard information. This is true, but they allow to represent a text more compactly and thereby reduce the noise of the representation. We explain those preprocessing steps and the bag of words model with a more illustrative example.

\subsection*{Bag of Words Example}
Assume the corpus consists of two documents $d_{1}=$ ``The patients with disease A.'' and $d_{2}=$ ``The patients with disease B.''. After tokenization, removal of stop words, and stemming, the bag of words vectors representing the two texts are:

\[
\textbf{v}_{d_1}=\begin{array}{c}
1\\
1\\
1\\
0
\end{array}\ \textbf{v}_{d_2}=\begin{array}{c}
1\\
1\\
0\\
1
\end{array}\ \begin{array}{c}
\rm{patient}\\
\rm{disease}\\
\rm{A}\\
\rm{B}\\
\end{array}
\]

Note how the punctuation character does not appear, as the tokenization has removed it from the list of words. The words ``the'' and ``with'' have been removed as stop words and the word ``patients'' has been stemmed to produce ``patient''. Then the list of words has been converted to a bag of words vector.

\subsection*{Application and shortcomings}
The vectors $\textbf{v}_{d_1}$ and $\textbf{v}_{d_2}$ live in a four dimensional vector space and we can either use them as feature vectors for a machine learning task or compute a measure of similarity between them. The standard similarity measure used for bag of words vectors is the cosine similarity $s_{\cos}(\textbf{v}_1, \textbf{v}_2)=\frac{\langle\textbf{v}_1,\textbf{v}_2\rangle}{||\textbf{v}_1||\cdot ||\textbf{v}_2||}$, (!correct! sollte ich hier auch $\textbf{v}_{d_1}$ shcreiben oder ist $\textbf{v}_1$ ok?) where $\langle\textbf{v}_1,\textbf{v}_2\rangle$ is the scalar product of $\textbf{v}_1$ and $\textbf{v}_2$ and $||\textbf{v}||$ is the norm or length of vector $\textbf{v}$. The cosine similarity has the appealing property that it normalizes the scalar product by the norm of the vectors used. Thereby it ensures, that the resulting measure lies in the interval of $[-1,1]$. For positive spaces (i.e. spaces, where all vectors have only non-negative elements, like the bag of words vector space) this measure lies in the interval $[0,1]$. Two documents $d_{1}$ and $d_{2}$ are then considered to be more similar than $d_{3}$ and $d_{4}$, if $s_{\cos}(\textbf{v}_{d_1}, \textbf{v}_{d_2}) > s_{\cos}(\textbf{v}_{d_3}, \textbf{v}_{d_4})$.

Generally, however, the bag of words model has severe limitations. Consider for example
the two sentences $d_{3}=$ ``The patients with disease A, but not B'' and $d_{4}=$ ``The patients with disease B, but not A''. Their representation in the bag of words model is equivalent, although they express quite different meanings. Additionally, some phrases that express similar or identical meaning are treated very differently in the bag of words model. Consider the example disease word ``Chronic-lymphocytic-leukemia''. Many doctors will abbreviate this word to ``CLL'' or ``B-CLL''. The bag of words model regards these three forms of the same word as completely dissimilar. Even worse, when spelled ``Chronic lymphocytic leukemia'', the phrase is regarded as three distinct words, having nothing in common with each other or the words above.
Still the bag of words model is a standard approach for information retrieval problems, as
it has many desirable properties. Texts can be straight forwardly embedded in a vector space (i.e. represented as vectors) and these embeddings can be used as fixed length feature representations for machine learning tasks.
% In information retrieval often
%the focus lies on the topic of a text rather than it's specific content.
%Some of the weaknesses of the bag of words approach are then neglectable.
%Consider again the sentences $d_{3}$ and $d_{4}$ and additionally
%the sentence $d_{5}=$''computers automate tasks''. If we are interested
%in the topic of the sentences or in their relevance to each other,
%then the bag of words model will give the reasonable result that $d_{3}$
%and $d_{4}$ are more relevant to each other than to $d_{5}$.



\section{Duplicate Detection}

With the bag of words model and the longest common subsequence matching method we semi-automatically identify duplicates in our data. We had a senior oncologist identify all duplicates present in a subset of 150 of these letters manually to have a baseline to which we can compare the methods' performances.

The longest common subsequence matching procedure scans two documents and finds the longest sequence of characters they have in common. If the length of this sequence exceeds a threshold the pair is marked as possibly duplicate and manually inspected. To get more hits we lower the threshold until the false positive rate becomes to high. Secondly we preprocess all texts as described above and map them into the bag of words representation. We use the cosine similarity as a measure of relatedness between pairs and mark all pairs with distance lower than a threshold as possibly duplicate and again iteratively decrease the threshold.

The bag of words approach identifies all manually detected duplicates and follow-ups while having a very low false positive rate. It detects one additional manually undetected follow-up pair. The string matching procedure is not as useful due to a high false positive rate and does not detect all manually found duplicates. Its performance is worse than the performance of the bag of words procedure, especially for follow-up letters. However, it finds a second manually undetected follow-up pair, that we have not identified with the bag of words procedure either. We use both approaches with thresholds from the "training" set on the remaining 157 letters as well. Overall we detect 18 exact copy pairs and 17 follow-up pairs. We additionally remove three letters from the dataset as they include almost no information (an artifact of the specific documentation procedure at the university hospital). This results in our final dataset consisting of 269 unique physician letters (follow-ups are only included, where mentioned explicitly).

%To be able to show or hide specific parts of the letters we try to break them apart into useful sections. (Irgendwie kein runder Ãbergang. Bin mir auch nicht ganz sicher, ob der nÃ¤chste Teil noch in die Intro gehÃ¶rt. Macht aber schon Sinn, dann kann ich im nÃ¤chsten Kapitel gleich mit einer Anwendung anfangen, fÃ¼r die ich alle anderen Methoden einfÃ¼hren muss.)


%string matching:
%false positives = 13
%not found fu = 2
%not found copy outside 150 = 1
%not found fu outside 150 = 2


%Unfortunately 18 of the 307 letters are duplicates and thereby not usable for our
%analysis. We excluded another 3 letters, that contain almost no information
%about the patient (this kind of letter is produced due to the specific
%kind of documentation process at the clinic). Another 17 are ``follow-up''
%letters i.e. letters of the same patient at a later point in time.
%This left us with the letters of 269 individual patients (the follow-up
%letters were only used where stated explicitly). Note that it is not
%a trivial task to ensure that the set of letters contains neither
%duplicates nor follow-ups. Automated aids for this problem will be
%discussed below.

%For our goals of automatically extracting information from and finding similarities
%between the letters we additionally needed supervised information.
%For a subset of 135 letters we obtained supervised labels of two kinds.
%One, we manually labeled the letters for whether or not the patients
%suffered from specific diseases common among those patients. These
%labels were checked by an expert %Prof. Mertelsmann
%for correctness. Two, %Prof. Mertelsmann
%we obtained a grouping of these 135 patients into 50 non-overlapping groups from an expert.
%This grouping is meant to reflect the expert's
%intuition about similarities between patients, that might not be capturable
%in simple rule based grouping approaches like grouping by disease. Additionally we performed a psychological experiment with {[}replace{]} medicine students and {[}replace{]} doctors, probing their intuition about letter similarity for a larger set. With these data we tried several
%approaches for automatic extraction of information from the letters
%and automatic similarity ratings between them. The methods used in our approaches are described in the next chapter.


\end{document}