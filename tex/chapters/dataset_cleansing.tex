% !TEX root = ../Thesis.tex
\begin{document}
\documentclass[Thesis.tex]{subfiles}
\chapter{Dataset Cleansing}
\label{ch:dataset cleansing}


The dataset we acquired from the university hospital contains several duplicate letters. To ensure undistorted test results, we have to identify as many of them as possible. It is clear that finding all of these duplicates manually is not only error-prone, but also very time consuming as in principle $\frac{(n-1)^2 + (n-1)}{2}$ letter comparison have to be made. With $n=307$ this amounts to almost 50,000 comparisons. Additionally personal information useful for this task, like names and birthdates, has been removed during anonymization. Therefore we make use of two semi-automatic procedures for duplicate identification. First we use a longest common subsequence matching method. This method would be sufficient for our problems, if we only had to deal with exact duplicates. However, we face the issue of follow-up letters with modified information in our dataset. To overcome this problem we use a procedure well known in the information retrieval community: The bag of words model for representing texts as vectors, with which it is possible to compute distances between documents. Duplicates are then found by their vector proximity. 
%We realized that several duplicate letters are present in this dataset, however. So to identify all of them we make use of two semi-automatic procedures for identifying duplicates. First we use a longest common subsequence string matching method. Second we represent the documents in the bag of words model, where distances between documents can be computed, and find duplicates by their proximity.


\section{The Bag of Words Model}
The bag of words model represents a text as the multiset (bag) of its words. That means all word
order information is disregarded and only the information how often a word is present in a text is encoded \citep{Manning2008vect}. (Mit der Art wie das Buch hier zitiert wird bin ich noch nicht glücklich. Es sollte eigentlich chapter 2 und 6 sein und nicht 2008 a und b.)
More concretely in the bag of words model documents are represented as fixed length
feature vectors, where every feature is a word occurrence
count. In the simplest approach every feature is the word or term frequency
tf$(t,d)$ of term $t$ in document $d$. Where tf$(t,d)$ is
the occurrence count of word or term $t$ in document $d$. To compute feature
vectors for documents in a corpus the vocabulary $V$ of the corpus
needs to be established first. Documents are represented by a $1\times|V|$
vector of the values tf$(t,d)$ for all $t\in V$.
To represent text in the bag of words model, however, the text needs to be preprocessed first.

\subsection*{Preprocessing}
In computers text is most often represented as sequences of characters. The process of extracting words from such a sequence is known as tokenization \citep{Manning2008prepr}. A first naive approach to tokenization might be to
split the sequence of characters on every whitespace and regard everything
in between as a word. This approach, however, can easily lead to unexpected
results. Consider the example string ``The doctor treats the patient.''. The naive
approach yields the words ``The'', ``doctor'', ``treats'', ``the'' and ``patient.''.
Note how the last word contains the punctuation character ``.''.
So even for very easy examples the naive approach does not produce
expected results. Luckily many more sophisticated algorithms are implemented
in ready-to-use open-source software packages.

Some words are more important or representative of a text than others and so it is a useful preprocessing step for the bag of words model to remove some frequently appearing but uninformative words, so called stop words. A list of English stop words comprises words like ``the'', ``a'', ``just'' or ``some''.  While they are necessary for the
grammatical structure of a language, they do not convey much information
about the similarity of documents \citep{Manning2008prepr}.

Additionally for the bag of words model we are not interested in the exact grammatical form in which a word is present in a text. Removing this unnecessary information is achieved with a procedure called stemming, that maps word appearances to their stem. It maps words such as ``doctors'' and ``doctor'' both to their common stem ``doctor''. Both removal of stop words and stemming seem at first glance to disregard information. This is true, but they allow to represent a text more compactly and thereby reduce the noise of the representation. The following example might provide clarification about the procedures.

\subsection*{Bag of Words Example}
Assume the corpus
consists of two documents $d_{1}=$``The patients with disease A.'' and $d_{2}=$``The patients with disease B.''. After tokenization, removal of stop words and stemming the vectors representing the two texts are:

\[
\textbf{v}_{d_1}=\begin{array}{c}
1\\
1\\
1\\
0
\end{array}\ \textbf{v}_{d_2}=\begin{array}{c}
1\\
1\\
0\\
1
\end{array}\ \begin{array}{c}
\rm{patient}\\
\rm{disease}\\
\rm{A}\\
\rm{B}\\
\end{array}
\]

Note how the punctuation character does not appear, as the tokenization has removed it from the list of words. The words ``the'' and ``with'' have been removed as stop words and the word ``patients'' has been stemmed to produce ``patient''. Then the list of words has been converted to a bag of words vector.

\subsection*{Application and shortcomings}
The vectors $\textbf{v}_{d_1}$ and $\textbf{v}_{d_2}$ live in a four dimensional vector space and we can either use them as feature vectors for a machine learning task or compute the distance between them to get a measure of dissimilarity. The standard distance measure used for bag of words vectors is the cosine distance $d_{\cos}(\textbf{v}_1, \textbf{v}_2)=\frac{\textbf{v}_1\cdot \textbf{v}_2}{||\textbf{v}_1||\cdot ||\textbf{v}_2||}$, where $\textbf{v}_1\cdot \textbf{v}_2$ is the scalar product of $\textbf{v}_1$ and $\textbf{v}_2$. This distance measure is preferred over the standard euclidean distance $d_{\rm{euclid}}(\textbf{v}_1, \textbf{v}_2)=\sqrt{\textbf{v}_1\cdot \textbf{v}_2}$, as it normalizes the scalar product, by the product of the lengths of the vectors. Thereby only the relative word frequencies are important and not the absolute ones. I.e. pairs of longer documents do not have a bigger distance, just because the absolute word counts are higher. 

Generally the bag of words model has severe limitations. Consider for example
the two sentences $d_{3}=$``The patients with disease A, but not B'' and $d_{4}$=``The patients with disease B, but not A''. Their representation in the bag of words model is equivalent,
although they express quite different meanings. Still the bag of words
model is a standard approach for information retrieval problems, as
it has many desirable properties such as the fixed length representation and straight forward embedding of texts.
% In information retrieval often
%the focus lies on the topic of a text rather than it's specific content.
%Some of the weaknesses of the bag of words approach are then neglectable.
%Consider again the sentences $d_{3}$ and $d_{4}$ and additionally
%the sentence $d_{5}=$''computers automate tasks''. If we are interested
%in the topic of the sentences or in their relevance to each other,
%then the bag of words model will give the reasonable result that $d_{3}$
%and $d_{4}$ are more relevant to each other than to $d_{5}$.



\section{Duplicate Detection}

With the bag of words model and the longest common subsequence matching method we semi-automatically identify duplicates in our data. We had an expert identify all duplicates present in a subset of 150 of these letters manually to have a baseline to which we can compare the methods' performances.

The longest common subsequence matching procedure scans two documents and finds the longest sequence of characters they have in common. If the length of this sequence exceeds a threshold the pair is marked as possibly duplicate and manually inspected. We lower the threshold until the false positive rate becomes to high. Secondly we preprocess all texts as described above and map them into the bag of words representation. We use the cosine distance to compute a measure of dissimilarity between pairs and mark all pairs with distance lower than a threshold as possibly duplicate and again iteratively increase the threshold.

The bag of words approach identifies all manually detected duplicates and follow-ups while having a very low false positive rate. It detects one additional manually undetected follow-up pair. The string matching procedure is not as useful due to a high false positive rate and does not detect all manually found duplicates. Its performance is worse than the performance of the bag of words procedure, especially for follow-up letters. However, it finds a second manually undetected follow-up pair, that we have not identified with the bag of words procedure either. We use both approaches with thresholds from the "training" set on the remaining 157 letters as well. Overall we detect 18 exact copy pairs and 17 follow-up pairs. We additionally remove three letters from the dataset as they include almost no information (an artifact of the specific documentation procedure at the university hospital). This results in our final dataset consisting of 269 unique physician letters (follow-ups are only included, where mentioned explicitly).

%To be able to show or hide specific parts of the letters we try to break them apart into useful sections. (Irgendwie kein runder Übergang. Bin mir auch nicht ganz sicher, ob der nächste Teil noch in die Intro gehört. Macht aber schon Sinn, dann kann ich im nächsten Kapitel gleich mit einer Anwendung anfangen, für die ich alle anderen Methoden einführen muss.)


%string matching:
%false positives = 13
%not found fu = 2
%not found copy outside 150 = 1
%not found fu outside 150 = 2


%Unfortunately 18 of the 307 letters are duplicates and thereby not usable for our
%analysis. We excluded another 3 letters, that contain almost no information
%about the patient (this kind of letter is produced due to the specific
%kind of documentation process at the clinic). Another 17 are ``follow-up''
%letters i.e. letters of the same patient at a later point in time.
%This left us with the letters of 269 individual patients (the follow-up
%letters were only used where stated explicitly). Note that it is not
%a trivial task to ensure that the set of letters contains neither
%duplicates nor follow-ups. Automated aids for this problem will be
%discussed below.

%For our goals of automatically extracting information from and finding similarities
%between the letters we additionally needed supervised information.
%For a subset of 135 letters we obtained supervised labels of two kinds.
%One, we manually labeled the letters for whether or not the patients
%suffered from specific diseases common among those patients. These
%labels were checked by an expert %Prof. Mertelsmann
%for correctness. Two, %Prof. Mertelsmann
%we obtained a grouping of these 135 patients into 50 non-overlapping groups from an expert.
%This grouping is meant to reflect the expert's
%intuition about similarities between patients, that might not be capturable
%in simple rule based grouping approaches like grouping by disease. Additionally we performed a psychological experiment with {[}replace{]} medicine students and {[}replace{]} doctors, probing their intuition about letter similarity for a larger set. With these data we tried several
%approaches for automatic extraction of information from the letters
%and automatic similarity ratings between them. The methods used in our approaches are described in the next chapter.


\end{document}