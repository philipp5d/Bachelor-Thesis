% !TEX root = ../Thesis.tex
\begin{document}
\documentclass[Thesis.tex]{subfiles}
\chapter{Recommender System}
\label{ch:recommender_system}

The main objective of this thesis is to build a prototypical recommender system for the physician letters and assess its quality. We do so based on the document embedding methods introduced in chapters \ref{ch:dataset cleansing} and \ref{ch:infpres}. Once documents are embedded in a vector space similarities between documents $d_1$ and $d_2$ can be computed as the cosine similarity of the corresponding vectors $\rm{sim}_{\cos}(\textbf{v}_{d_1}, \textbf{v}_{d_2})$. To find the most relevant letters, given a reference letter, we compute the similarity of the reference letter to all other letters in the database. Thereby we get a ranking of all other letters, given the reference letter. The "best" fitting letter or the letter with rank one is considered the most relevant according to the algorithm. The $n$ most relevant letters are the ones on the $n$ highest ranks or the $n$ letters with highest cosine similarity to the current reference letter. They can be retrieved from the database and presented to a user.

\section{Fine-tuning}
Based on the cosine similarity between vectors of the corresponding texts all document embedding methods can in principle be used as a base for the recommender system. To find out which hyperparameters and which embedding method works best we use supervised similarity information. The most desirable information is an expert rating of similarity between letter pairs. As this information is expensive to come by, because experts working hours are expensive, we use a different, easier acquirable dataset for the task of fine-tuning. This data consists of a grouping of 135 of the letters into 50 non-overlapping groups of similar patients done by an expert. Some groups only contain a single patient (if he/she was dissimilar to all others), some contain several and the average group consists of 2.7 patients. This grouping is not equivalent to a correct measure of similarity, but we can still use it as an approximate measure of similarity to tune our algorithm. Thereby letters from the same group are considered similar and letters from different groups are considered dissimilar. One measure of goodness for a real recommender system is how often the system ranks the truly similar letters into the top $n$. I.e. given one reference letter, are the similar letters of the grouping dataset considered to be among the top $n$ most relevant by the recommender system.  We call this the top-n measure. This measure, however, does not take all available information into account. Say we specify $n=5$, the top-n measure gives the same score, if a truly similar letter is ranked to be the 6th most similar or the least similar of all. A measure that assigns a higher score in the first situation than in the second would be preferable for the fine-tuning of the algorithm. We therefore develop a continuous measure, that assigns a score from the interval $[0,1]$ for all possible ranking situations. A score of 1 is given, if the truly similar letters occupy the foremost positions, a score of 0, if the truly similar letters occupy the last positions, a score of 0.5, if the truly similars occupy the positions in the centre. A score of 0.5 is expected, if the algorithm sorted the letters by chance.

Based on the continous measure we first do hyperparameter tuning for the embedding methods as applicable. Afterwards we select the best performing embedding method the same way. Table \ref{table:continuous_measure} shows the scores obtained by each embedding method. Generally the performance is high above chance level. It is noteworthy that the simple and hyperparameterless tf-idf method outperforms all other methods including LDA and the recent and hard-to-tune paragraph vector. We therefore choose tf-idf as the embedding method for the recommender system. 

There is a relatively intuitive reason why tf-idf works well for our problem. It is plausible that the main features for human similarity judgments of letter pairs are based on the diagnosis of the patients. The diagnosis and diseases words are features that the tf-idf method will judge as very important, as they appear only in few documents overall. %An additional problem, however, is that of different spellings of the same disease. Consider for example the disease "chronic lymphocytic leukemia". Spellings range from "CLL" over "B-CLL" to "chronic-lymphocytic-leukemia". All these spellings are regarded by all our models as separate entities. However, the tf-idf vectors are still useful document embeddings, as the overall word statistics still differ depending on the diagnosis. Words describing medication for leukemia for example are still present in all letters of patients with CLL. These medication words, however, are again present only in few documents overall and are thereby considered more important features by the tf-idf method.
\begin{table}
	\begin{tabular}{|c||c|c|c|c|c|}
		\hline 
		Embedding Method & BOW & TF-IDF & LSA & LDA  & Para2Vec\tabularnewline
		\hline 
		\hline 
		Continous Measure Score & 0.794 & \underline{0.870} & 0.868 & 0.634 & 0.830\tabularnewline
		\hline 
	\end{tabular}
	\caption{Performance of different embedding methods (with tuned hyperparameters) on the grouping dataset evaluated with the continous measure.}
	\label{table:continuous_measure}
\end{table}



\section{Recommendation Quality---Experimental Setup}
Having fine-tuned the recommendation procedure as described above the next step consists of assessing the quality of the recommendation. We test this quality in a psychological experiment. To this end we probe the similarity ratings that subjects give to pairs of letters and compare them to the similarity measure of our algorithm.

We construct an experiment with 32 trials, in which subjects have to compare letter pairs for similarity. More precisely that means we select 32 letters as ``reference letters'' out of our database and let subjects rate the similarity of these 32 reference letters to five other letters each. Thereby we gain ratings for 160 letter pairs. 16 of the reference letters have a follow-up letter in our database. Trials with this kind of reference letter are called ``follow-up trials'' and letter pairs of follow-ups are called ``follow-up pairs''. The other 16 reference letters are selected randomly among the letters without follow-ups present in our dataset. The five letters that are compared to one reference letter are called the comparison letters. Four of them are selected based on the cosine similarity between the reference letter and all other letters. These four are the ones with highest cosine similarity to the reference letter according to our algorithm. That means they are the best matches to the reference letter according to our algorithm---or equivalently, they are ranked on places one to four given a reference letter. The fifth comparison letter is randomly selected among all other letters and then fixed. Subjects are presented with one reference and one comparison letter at a time. After rating their similarity they are presented with the next comparison letter. Once a trial is done, i.e. five comparison letters are rated, the next reference letter is presented. The order of the reference and comparison letters is random, but fixed. Subjects are forced to give a rating in the range of 1 (very dissimilar) to 7 (very similar) for each letter pair. See figures \ref{fig:whole_webexperiment}, \ref{fig:webexperiment_left}, and \ref{fig:webexperiment_right} (in the Appendix) for screenshots of the website on which the subjects perform the experiment.

We design the experiment such that the first trial is a follow-up trial and the second one is a non-follow-up trial. Thereby we ensure that subjects can adjust for the upper similarity bound of having to compare letters of the same patient. These two trials are excluded for the later analysis. We have six subjects performing the experiment, four experts (oncologists with at least five years of practical experience) and two novices (medical students more than halfway through their study course).


%\begin{figure}
%	\includegraphics[width=\linewidth]{figures/webexperiment_screenshot2}
%	\caption{Screenshot of the website view during the experiment.}
%	\label{fig:webexperiment}
%\end{figure}



%We probe the similarity ratings that subjects give to pairs of letters and compare them to the similarity measure of our algorithm. We select 32 letters out of our database, 16 of them have a follow-up letter in our database, the other 16 are randomly selected among the ones without follow-up letter. We take these 32 as reference letters and select to each of them five other, so called comparison letters, with which they will be compared for similarity. Four of these five are the ones with highest cosine similarity to the reference letter as computed with our algorithm. The fifth is randomly selected and then fixed. Subjects are presented with a reference letter and the five comparison letters in a random, but fixed sequence. They are forced to give a rating for each pair in the range of 1 (very dissimilar) to 7 (very similar) and can then move on to the next reference letter. See figure \ref{fig:webexperiment} for a screens hot of the website on which the subjects perform the experiment. We ensure that among the first two reference letters is one follow-up reference letter and one non-follow-up reference letter. This way subjects can adjust for the upper similarity bound of having a letter pair with both letters from the same patient. In total we obtain ratings for 160 letter pairs from each subject. However, the rating data for the first two reference letters is discarded for the later analysis, leaving 150 letter pairs for evaluation. Six subjects perform our experiment, four experts (oncologists with at least five years of practical experience) and two novices (medical students more than halfway through their study course).



\section{Recommendation Quality---Results}

\subsection*{Inter-Rater Agreement}

We first analyze the inter-rater agreement among subjects. We believe it is not trivial for them to rate physician letter pairs for similarity as it is not directly clear along which dimension similarity is to be judged, a problem well known from the cognitive science literature \citep{Medin1993}. One might for example judge similarity based on diagnosis or based on therapy. We also believe that experts and novices (or students) base their judgments on different features. Experts generally have higher agreement when categorizing stimuli, than novices do, and usually use more abstract, less accessible features \citep{Chi1981, Linhares2007, Leon-Villagra2013}. In line with these results we find that the inter-rater agreement is higher among the experts than among the students (expert agreement: 0.76; student agreement: 0.59). We measure this agreement with the spearman rank correlation coefficient, that produces a number between -1 (perfect anti-correlation) and 1 (perfect correlation). Spearman's coefficient is used instead of the often used cohen's kappa coefficient, because the former can deal with ordinal data, whereas the latter can only deal with nominal data \citep{Spearman1904, Cohen1960}. Our number of subjects is quite low for concluding that students rate letter pairs worse than experts. Still as our findings are in line with well established research, we discard the student rating data for further analysis. 

Note that for the following analyses we exclude data of follow-up pairs, except where explicitly stated otherwise. Subjects rate the similarity of these pairs very highly and almost any information retrieval system will find them to be similar. Thereby they would improve positive correlation statistics in our analysis, although retrieving them is useless in practice. We will show later that our recommender can easily distinguish them from normal pairs.

\subsection*{Ranking and Subject Ratings}

The first analysis concerning our recommender system asks the question whether the recommendations are better than chance. Therefore we compare the average subject rating of the "best fitting" letter (as computed with our algorithm) and the random comparison letter for each trial. The best fitting comparison letter is the one with highest cosine similarity to the reference letter. % with highest cosine similarity to a reference letter (i.e. the "best fitting" letter or the letter with rank one, according to our algorithm) to the rating of the randomly chosen comparison letter for each trial.
Figure \ref{fig:both_diff_tf}a shows a histogram of the differences in subject rating of the best fitting and the randomly selected letter pairs. The figure also shows the mean difference and a 95\% interval for this mean. The confidence interval was calculated using the standard error of the mean (sem). The lower and upper bound is then calculated as $bound_{1/2} = mean \pm 1.96 * sem$ respectively. This estimation of the confidence interval, however relies on the assumption that sample means are normally distributed. As those means are calculated over 30 values in each sample, this assumption is hardly quesitionable in most scenarios due to the central limit theorem. As the differences in subject ratings are discrete values bounded between -6 and +6 we might face the issue of a skewed distribution because of bounding problems, though. Therefore we verify the assumption of normality with bootstrapping \citep{Efron1979}.
\begin{figure}[h]
	\includegraphics[width=\textwidth]{figures/both_diff_tf}
	\caption{\textbf{(a):} Histogram of differences in rating of the "best" and the random comparison letter for each trial.
		\textbf{(b):} Histogram of the average differences in averaged subject ratings of the two groups for 100.000 bootstrap resampled datasets.}
	\label{fig:both_diff_tf}
\end{figure}
 %the means and standard deviations of the average rating that subjects gave to the "best" and randomly selected letters. One sees that letter pairs with highest cosine similarity are rated higher than the randomly chosen ones. However, both groups show a high standard deviation. For the group of letters selected by our algorithm this might be caused the following way. Often the algorithm can find a good match and the rating of subjects will be high. However, sometimes a good match is just not found and the rating is low. This can happen due to several reasons. First our algorithm might just not find the fitting letter from our database. But second our small dataset might just not include a fitting letter to a reference letter. In this case the best letter we can find in principle is still rated low by subjects. We will address this problem further, when discussing the connection between letter pair cosine similarity and subject rating more thoroughly. 
(!correct! der Table sollte wirklich noch auf der Seite vorher kommen, vor der Figure.)

Bootstrapping is a procedure in which many random resamples of the dataset are computed. If our collected dataset represents the true distribution of differences in ratings of best fitting and random letter pairs well enough, then the resamples are to our sample as our sample is to the true distribution. Thereby the mean of the average difference of the resampled datasets give an estimate of the average differences, when collecting many samples from the real distribution. We take 100.000 resamples with replacement from our data and compute the average difference of the resamples for every one of them. The distribution of these average differences of the resamples can be seen in figure \ref{fig:both_diff_tf}b. From this figure it is apparent, that the assumption of normality for the distribution of average differences indeed holds. The figure also shows bootstrap estimates for the mean and the 95\% confidence interval. As expected these estimations match closely with the ones estimated with the standard error of the mean as can be seen from the following table.

\centering
\begin{tabular}[t]{|c||c|c|}
	\hline 
	Estimated Statistic & Mean & Confidence Interval\tabularnewline
	\hline 
	\hline 
	Standard Estimation & 1.93 & [1.17, 2.70]\tabularnewline
	\hline 
	Bootstrap Estimation & 1.93 & [1.14, 2.68]\tabularnewline
	\hline
\end{tabular}

\justifying
%\caption{Standard and bootstrap estimation of statistics of interest about the distribution of average pairwise differences of the "best" and the random comparison letter.}
%\label{table:statsoi_tfidf}


\subsection*{Cosine Similarity and Subject Ratings}


\begin{wrapfigure}{r}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.98\linewidth]{figures/rating_vs_sim_mean}
	\end{center}
	\caption{Averaged cosine similarity of all letter pairs that were rated into one of the seven possible rating categories.}
	\label{fig:rating_vs_sim}
\end{wrapfigure} 
After analyzing whether ranking letter pairs with our algorithm works better than chance, we analyze the relationship of the cosine similarity and the average subject ratings of letter pairs more directly. We visualize the mean cosine similarity, that our algorithm assigns to pairs, as a function of the subject rating in figure \ref{fig:rating_vs_sim}. The data shows a positive, close to linear correlation (pearson correlation: 0.96) of those two variables, suggesting that our recommendation method captures at least some aspects of perceived similarity.

%\begin{SCfigure}
%	\centering
%	
%		
%\end{SCfigure}
Next we analyze the relationship of the cosine similarity and the subject ratings more thoroughly. We therefore draw all letter pairs as points in a plot of average rating of a pair versus cosine similarity of a pair. See figure \ref{fig:all_points} for this visualization. We mark follow-up pairs green and the random comparison letter pairs red. From the figure it is apparent that follow-up letters can be easily distinguished with the cosine similarity from other letter pairs. The randomly selected pairs mostly have a low cosine similarity as expected, however several of them are still rated rather highly by subjects. Some well fitting random letter pairs are expected by chance and after manual inspection of these pairs we settle on this conclusion.




\begin{figure}
	\includegraphics[width=\linewidth]{figures/rating_vs_similarity}
	\caption{Average subject rating versus cosine similarity of all letter pairs. Follow-up pairs are marked green, random ones red.}
	\label{fig:all_points}
\end{figure}


\subsection*{Considerations of Precision and Recall}
A standard way to assess the quality of an information retrieval system uses two measures called precision and recall. Precision and recall are used as measures of the goodness of an information retrieval system, when all documents are binarily classified as either relevant or irrelevant given a current information need or query \citep{Manning2008prerec}. In our case a query is a reference letter and we ask the question of whether a comparison letter is relevant or irrelevant given this reference letter. Precision is then a measure of the correctness of the retrieved results. In probabilistic terms it is the probability that a document is relevant given that it is retrieved $\rm{P}(relevant|retrieved)$. Recall is a measure of completeness of the retrieved results and can be expressed as the probability of being retrieved given that the document is relevant P$(retrieved|relevant)$. Both quantities usually exhibit an inverse relationship. One can trade higher precision for lower recall or vice versa. Both precision and recall always lie in $[0,1]$. Note that we classify letter pairs with an average rating of five or higher as relevant and others as irrelevant. This threshold is somewhat arbitrary, but was set after several personal discussions with the contributing physicians.

\begin{figure}
	\includegraphics[width=\linewidth]{figures/precision_rank_sim}
	\caption{\textbf{(a):} Precision as a functin of rank cut-off. \textbf{(b):} Precision as function of a varying cosine similarity threshold. The x-axis shows the average number of retrieved letters per reference letter at the threshold instead of the threshold itself.\\
	Note that both plots start at the same point, where all letters with rank four or lower are retrieved, and share a common y-axis.}
	\label{fig:precision}
\end{figure}

We can assess the precision of our system in different scenarios. Recall, however, is impossible for us to measure. We do not know the set of relevant letters for a given information need (i.e. reference letter) and therefore cannot compute P$(retrieved|relevant)$ (i.e. recall). Precision is a quantity of high interest for the recommender system in the use case we envision. It is very important for a setting in clinical everyday life, that a large fraction of recommended letters is deemed useful by doctors. If this were not the case, physicians would quickly stop using the system. Recall on the other hand is not an interesting quantity in this situation. Doctors will only look at very few recommended letters and it is of no concern whether or not a large fraction of relevant documents is retrieved. However, an interesting question, somewhat related to recall, is whether or not the most relevant letters are retrieved first. We will address this question after examining the precision of our system. Note that for another use case recall might be more important than precision. 

In our usage scenario a possible implementation of the system could always retrieve the four highest ranked letters to a given reference letter. Therefore we ask the question of what precision we can achieve, when recommending the "best" four or even fewer letters. Figure \ref{fig:precision}a shows what level of precision the system can achieve, when stopping retrieval of letters at a given rank. As is apparent from this figure precision is rather low at a level of roughly 0.55 when presenting all four "best" matches. Unfortunately not much increase in precision can be gained by presenting a fixed number of fewer letters than four, with a maximal precision of 0.59 when presenting the two letters with highest cosine similarity. This means that little more than every second retrieved letter is relevant. From figure \ref{fig:all_points} it is apparent, however, that we can do better, when incorporating information about the absolute cosine similarity and not only the ranking of the letters. In this figure it is apparent, that we can get up to a precision of 1, if we present only letters with a cosine similarity higher than 0.4. This in turn means that only for few reference letters some comparison letters are retrieved. In figure \ref{fig:precision}b we examine this relationship more closely. We restrict ourselves to letters of rank four or lower and plot the precision of retrieved letters as a function of a varying cosine similarity threshold. Instead of labeling the x-axis with this threshold we show the average number of retrieved letters per query or reference letter. Thereby we can visualize the trade-off between higher precision and fewer retrieved letters. Increasing precision this way is much easier achievable. This though comes with the cost that for some reference letters no comparison letters might be retrieved. Unfortunately we believe that the recommender system is of highest value to doctors in cases of very uncommon patients. In those cases, however, it is probably least likely to find other letters with high cosine similarity to the current one in the database. Therefore the very promising results of figure \ref{fig:precision}b have to be taken with a grain of salt for real application scenarios.

Next we turn to the question whether the most relevant letters are retrieved first. This question is hard to answer, since we only know of five letters how relevant they are to a reference letter. It remains unclear what rating subjects would have assigned to the other letters from our database. A related question though can provide some insight into this problem. We look at how well the ranking of the algorithm corresponds to the ranking of the experts, given by their rating. We compare these two rankings with the spearman rank correlation coefficient. The correlation between the ranking given by the average rating and the ranking of the algorithm is 0.39. While this is not particularly high, it is still far better than chance (spearman coefficient of 0). The inter-rater agreement among the experts when calculated accordingly (i.e. without follow-up pairs) is 0.72. Summarizing one can state that our systems ranking is well above chance, but still quite far below human gold standard.
% precision and recall. we can measure precision, but not recall. Precision is informative for our system, recall not so much, but retrievieng the most relevant letter would be interesting.
% first precision. Precision for ranks -> this is how the system could be used in practice. This looks rather bad. 
% precision after cosine similarity. This looks really good. Could for example retrieve in practice a maximal number of results k, but less of them, if they have too low cosine similarity. However, in this plot we are not looking at a representative sample of letter pairs over cosine similarity. Its mainly the letters with highest cosine similarity for an information need. Therefore our plot might not perfectly represent the true precision vs. cosine similarity. But this would be same in use case descriebed above. And also wide range of cosine similarity is covered extensively.

% more about recall:
% recall is impossible to measure, but also not very useful. We do not care in this setting wether or not all relevant letters are retrieved. We only look at the first k results anyway. What we do care about though, is whether the most relevant letters for any information need are retrieved. From looking at scatter plot it seems, that it is not necessarily the ones with highes cosine similarity, that get rated best by subjects. We measure the correlation of expert ranking of the letters with algorithm ranking. We find that with spearman rank only 0.39. This seems poor, but when comparing the interrater agreement of rankings (not ratings like above) we find thath expert interrater agreement is only at 0.45 (when including students 0.40). So the ranking of our system agrees among the first 4 plus one random letter quite well compared to expert agreement gold standard. We therefore can expect, that the most relevant letters are mostly retrieved first.




%The first question we have to consider, when analyzing precision and recall of our system, is asking when a document is considered relevant. We set the threshold for being relevant (somewhat arbitrarily) at an average subject rating of 5 or higher for a given letter pair. That this is a reasonable choice was reinforced through personal discussions with the collaborating doctors. We believe that for the systems use case in a clinic precision is of greater relevance than recall. If doctors are to read the retrieved patient letters regularly in the stressful clinical everyday life, the quality of retrieved results has to be high. This ensures that users find added value in the system and do not discontinue using it because of frustration. If one were using the system for a different purpose, say a research setting in which all patients with similar, extremely rare symptoms are to be found, recall was more important than precision. From figure \ref{fig:all_points} it is now apparent that it is relatively easy to use our system in a way that ensures high precision. If one were for example retrieving only letters with a cosine similarity higher than 0.35, not a single retrieved document would be irrelevant leading to a precision of 1. This comes at the cost of a low recall of 0.25. For a cosine similarity threshold of 0.23, precision is 0.89 and recall 0.57. These numbers have to be taken with a grain of salt. The thresholds were set manually after seeing the data and are therefore overfitted to this particular data. See figure \ref{fig:precision_recall_curve} for a visualization of the interpolated precision recall curve, where one can see all trade-offs between precision and recall. However, these numbers are also somewhat inaccurate. We mainly consider letter pairs found to be among the "best" with our algorithm for this plot. Thereby the selection of pairs is not representative for random samples taken from all possible pairs. Still our sample data covers a wide range of possible cosine similarity values. Therefore we still believe that precision and recall measures as reported here give a useful approximation of the true values.  (!correct! Man könnte den recall über die zufällig ausgewählten pärchen schätzen. In unserem Experiment sind dabei aber 4 von 30 zufälligen relevant, was auf das ganze Datenset gerechnet hieße, dass pro Referenzbrief 40 relevante Briefe vorhanden sind. Das ist aber sicherlich falsch! Und würde auch ganz komische recall zahlen ergeben.)


\subsection*{Considerations for further Work}

We have two reasons to believe that our system performs even better in reality than expected from the considerations above. First, when extending our recommender to work on a much larger database, we expect that the best fitting letters to a given reference letter will have much higher cosine similarity, than is the case now. This is because in a much larger database more well fitting documents are expected for each reference letter. For figure \ref{fig:all_points} this means, that we would expect almost no blue points on the left and many more in the region around a cosine similarity of 0.4. These points most likely will be rated highly by subjects, as are all points with such a cosine similarity in our experiment. In figure \ref{fig:precision}a and \ref{fig:precision}b this effect probably shifts both curves up higher. Second there is reason to believe that some of the letter pairs with low cosine similarity, but high subject rating are due to psychological adjusting. If during one trial only rather badly fitting letters are presented, we believe that subjects adjust their rating scale and rate partially fitting letters higher than normally. Effects like this are often observable in psychological experiments \citep{Poulton1975}. Therefore we believe that points in the upper left corner of figure \ref{fig:all_points} are not as problematic as it might seem. In the figure it looks like some are highly relevant although they have very low cosine similarity and will therefore probably not be retrieved in practice. If the assumption, that subjects adjust their rating scales, is correct, then some of the letters in this region are probably less relevant than it seems from our data.

\begin{figure}
	\includegraphics[width=\textwidth]{figures/both_diff_pv}
	
	\caption{\textbf{(a):} Histogram of differences in rating of the "best" and the random comparison letter for each trial.
		\textbf{(b):} Histogram of the differences in the means of averaged subject ratings of the two groups for 100.000 bootstrap resampled datasets.}
	\label{fig:both_diff_pv}
\end{figure}

Finally one can ask whether there are possibilities to improve the system further. The tf-idf embedding method does not have hyperparameters, so no further tuning of those is possible. We believe, however, that combining similarity measures from several embedding approaches is useful. Of the five embedding methods we tested only paragraph vector is a promising option for improving the results of the retrieval. Bag of words is more or less the simple basis for tf-idf, LSA is highly correlated with tf-idf as it is a feature compressed version of the latter, LDA performed severely worse than all other methods, leaving only paragraph vector. This method performed almost as well as tf-idf during our fine-tuning and computes feature vectors in a very different way, possibly complementing tf-idf well. Indeed we find cues that combining these two methods yields substantially better results. We again compare the ratings given to letter pairs from two groups. One of the groups consists of the letters from our experimental dataset, that paragraph vector ranks on the first place out of all letters from our complete database. I.e. letters in this group are ranked among the top four by tf-idf and top one by paragraph vector in our whole dataset. We only find 9 of these letters out of the 30 possible trials. The other group consists of the 9 randomly selected letters from the corresponding trials. Again we look at a histogram of differences in rating in figure \ref{fig:both_diff_pv}a and the distribution of average differences for 100.000 bootstrap resamples in figure \ref{fig:both_diff_pv}b. First, one has to note that neither the bootstrap nor the standard calculated means and confidence intervals are to be trusted completely at a sample size of 9. Additionally, one can see from figure \ref{fig:both_diff_pv}b that the assumption of normality does not hold perfectly this time. Indeed we face an upward bounding issue and the distribution's left tail is longer, than the right tail. Due to the violation of the assumption of normality, the confidence interval estimates differ. Comparing the mean and confidence interval estimates with figures \ref{fig:both_diff_tf}a and \ref{fig:both_diff_tf}b, where only tf-idf information was used, though, we see that additionally using information from the paragraph vector embedding improves results substantially. The mean of the average difference in ratings for example is improved from 1.93 to 3.69 units on the rating scale from 1 to 7. Therefore we conclude that using information from both embedding methods is highly useful, although we distrust the estimation of the mean to some degree due to the small sample size. In summary we believe that work addressing how to combine information from different embedding methods is a worthwhile task and should be undertaken by further research.
%Combining with Paragraph Vectors seems promising. Paragraph vector computes different, but also very useful features.
%Precision is already very good. And Precision seems to be more important for clinical every day life than recall. Recall might be more important for a research setting, but doctors will be quickly annoyed, if precision is low.
%Additionally we expect that, if we use a bigger database, we will have almost no datapoints in the low cosine similarity region. 
%Points high up-left might be due to subjects adjusting for the very low similarity in one trial, said one doctor.




\end{document}